{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Statistical Sampling- Unit 1 - NAEP statistics\"\nauthor: \"Alex Deshowitz\"\ndate: \"April 30, 2017\"\noutput: word_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\nThe dataframe used in this quick analysis was pulled from the NAEP nation's report card reports.  The data was pulled at the state level for 2009 - 2015.  The scores included are Math scores only.\n\nMore information regarding the data source can be found here : https://nces.ed.gov/nationsreportcard/\n\nThis very high level analysis covers the state of Texas and the changes in the scores for the state over the past 7 years.\n\n\n```{r}\n\n\npackages <- c('plyr', 'sqldf', 'reshape','ggplot2')\n\nsapply(packages, library, character.only = TRUE)\n\n```\n\n```{r}\n\n\noverall_df <- read.csv('Analysis/Data/Math_educ_overall_v01.csv', header = TRUE, skip = 9, nrows = 208)\n\n```\n\n```{r}\n\nhead(overall_df)\n\nncol(overall_df)\n\nstr(overall_df)\n\nsummary(overall_df)\n\n\n```\n\n\n\nThe first thing we need to do is create a better year column\n```{r}\n\nyears <- unique(overall_df$Year[!is.na(overall_df$Year)])\n\ny <- rep(years, each = 52)\n\noverall_df['new_year'] <- y\n\n# drop the old column\n\noverall_df$Year <- NULL\n\n```\n\n\nLet's reorder the dataframe now\n\n```{r}\n\noverall_df<- overall_df[,c(\"new_year\",\"Jurisdiction\",\"Average.scale.score\",\"Standard.Error\")]\n\n# rename\n\noverall_df <- rename (overall_df, c('new_year' = 'year','Jurisdiction' = 'jurisdiction', 'Average.scale.score' = 'avg_scale_score', 'Standard.Error' = 'std_error'))\n\n```\n\nIt looks like the standard error column is still funky; let's take a look at it\n\n```{r}\n\nstr(overall_df)\n\n\n```\n\nIt looks like our data is now good to go\n\n\nHow does Texas rank compared to other states in the country\n\n\n```{r}\n\n# order the data by score\n\nanalysis_df <- overall_df[order(overall_df$year, overall_df$avg_scale_score, decreasing =TRUE), ]\n\n\n# let'st just look at the 2015 values first:\n\nanalysis_df <- subset(analysis_df, year == 2015)\n\n# rank\n\nanalysis_df$rank<- NA\n\nanalysis_df$rank <- 1:nrow(analysis_df)\n\nhead(analysis_df, 53)\n\n\n\n```\n\n\n```{r}\n\n# let's look at the ranks of the math scores\n\nbarplot(analysis_df$avg_scale_score, names.arg = analysis_df$jurisdiction, cex.names = 0.7, las = 2, ylim = c(250,300), xpd = FALSE, main = \"2015 United States NAEP Math Scores\")\n\n```\n\nIt looks like Texas is ranked 25 in 2015.  I wonder if that has changed over the past several years?\n\n```{r}\n# need to rank and pull the data together for the previous years:\n\n# create subdfs (not the most efficient, but will be easy to rank)\n\nanalysis_df_2014 <- subset(overall_df, year == 2014)\nanalysis_df_2013 <- subset(overall_df, year == 2013)\nanalysis_df_2012 <- subset(overall_df, year == 2012)\nanalysis_df_2011 <- subset(overall_df, year == 2011)\nanalysis_df_2010 <- subset(overall_df, year == 2010)\nanalysis_df_2009 <- subset(overall_df, year == 2009)\n\nrm(analysis_df_2010, analysis_df_2012, analysis_df_2014)\n\n# now, we need to make sure that the ordering was maintainted\n\nhead(analysis_df_2013)\n\n# looks like it wasn't...let's resort all DFs\n\nanalysis_df_2013 <- analysis_df_2013[order(analysis_df_2013$year, analysis_df_2013$avg_scale_score, decreasing =TRUE), ]\n\nanalysis_df_2011 <- analysis_df_2011[order(analysis_df_2011$year, analysis_df_2011$avg_scale_score, decreasing =TRUE), ]\n\nanalysis_df_2009 <- analysis_df_2009[order(analysis_df_2009$year, analysis_df_2009$avg_scale_score, decreasing =TRUE), ]\n\n# looks like reordering worked well.  Now, let's add in the rank column:\n\nanalysis_df_2013$rank<- NA\nanalysis_df_2013$rank <- 1:nrow(analysis_df_2013)\n\n\nanalysis_df_2011$rank<- NA\nanalysis_df_2011$rank <- 1:nrow(analysis_df_2011)\n\nanalysis_df_2009$rank<- NA\nanalysis_df_2009$rank <- 1:nrow(analysis_df_2009)\n\n```\n\nNow, we should be good to go for a little more analysis.  We may want to consolidate all of this information into one dataframe just to be \"tidy\"\n\n\n```{r}\n\n# a very untidy approach, but works...Should really use the merge functionality in the Tidyverse\n\ntidy_df <- sqldf('Select T1.jurisdiction As jurisdiction, \nT1.rank As \"2015_rank\", \nT1.avg_scale_score As \"2015_avg_score\",\nT1.std_error As \"2015_std_error\",\n\nT4.rank As \"2013_rank\", \nT4.avg_scale_score As \"2013_avg_score\",\nT4.std_error As \"2013_std_error\",\n\nT3.rank As \"2011_rank\", \nT3.avg_scale_score As \"2011_avg_score\",\nT3.std_error As \"2011_std_error\",\n\nT2.rank As \"2009_rank\",\nT2.avg_scale_score As \"2009_avg_score\",\nT2.std_error As \"2009_std_error\"\n\n\nFrom analysis_df T1\n\nLeft Outer Join analysis_df_2009 T2 \nOn T1.jurisdiction = T2.jurisdiction\n\nLeft Outer Join analysis_df_2011 T3\nOn T1.jurisdiction = T3.jurisdiction\n\nLeft Outer Join analysis_df_2013 T4\nOn T1.jurisdiction = T4.jurisdiction')\n\n```\n\n\n```{r}\n\n# remove the unused dfs from the environment\n\nrm(analysis_df_2009, analysis_df_2011, analysis_df_2013)\n\n```\n\n\n\nLet's look at the scores for Texas over the past few years\n\n```{r}\n\npar(mfrow = c(1,2))\n\ntexas_df <- subset(tidy_df, tidy_df$jurisdiction == \"Texas\")\n\n# proly want to melt it \n\n\ntexas_df <- melt(texas_df)\n\ntexas_df <- sqldf('Select * From texas_df Where variable Like (\"%rank%\")')\n\ntexas_df$year <- c(2015, 2013, 2011, 2009)\n\nggplot(data = texas_df, aes(x = year, y = value)) + geom_line(color = 'dark blue', size = 2) + ggtitle('Texas Math Score Rank 2009-2015') + theme(plot.title = element_text(lineheight = 0.8, face = 'bold'))\n\n# do the same for score\n\ntexas_df <- subset(tidy_df, tidy_df$jurisdiction == \"Texas\")\n\ntexas_df <- melt(texas_df)\n\ntexas_df <- sqldf('Select * From texas_df Where variable Like (\"%score%\")')\n\ntexas_df$year <- c(2015, 2013, 2011, 2009)\n\nggplot(data = texas_df, aes(x = year, y = value)) + geom_line(color = 'dark blue', size = 2) + ggtitle('Texas Math Score 2009-2015') + theme(plot.title = element_text(lineheight = 0.8, face = 'bold'))\n\n```\n\nMaybe this is a little misleading, but what it shows is that the math score rankings for Texas have dropped over the past 7 years.  So, it looks like the real issue is that the Texas Math score has dropped over the past few years.  WE would need to look at some additional information to figure out why this is the case.  I wonder if the new score is statistically significantly different from 2009 levels though.  Remember that ggplot has automatically truncated the axis\n\n```{r}\n\n# change since 2009\n\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2015_avg_score' \n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2009_avg_score' \n\n\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2015_avg_score' /\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2009_avg_score' -1\n\n# peak score\n\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2011_avg_score'\n\n# difference from peak\n\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2015_avg_score' /\n(subset(tidy_df, tidy_df$jurisdiction == \"Texas\"))$'2011_avg_score' -1\n\n\n```\n\nThe score has only dropped 1% since 2009.  Granted, the peak score of 290 has seen a 2% decline.  I wonder if the testing procedures changed or something?\n\nLet's see if the difference between the years has a statistically significant difference.\n\n```{r}\n\n# let's look at error bars for the overlap possibilities usng the overall cleaned frame:\n\ntexas_df = subset(overall_df, overall_df$jurisdiction == 'Texas')\n\n# reorder\n\ntexas_df = sqldf('Select * From texas_df Order By year')\n\n# now plot the 95% confidence intervals: note that the std. error has already been calculated for us:\n\n\nggplot(texas_df, aes(x = year, color = year))+  geom_errorbar(aes(ymax = texas_df$avg_scale_score + 1.96*texas_df$std_error), ymin = texas_df$avg_scale_score - 1.96*texas_df$std_error, position = 'dodge', lwd = 1.5) + coord_flip(ylim = c(295, 275)) + ggtitle('Texas Math Score Confidence Intervals 2009-2015') \n\n\n\n```\n\n\nIt looks like the 2011 score is statistically significantly different from the 2015 score.  Of course, we are not making any adjustments such as bonferroni for multiple comparisons.  It does appear however, that 2015 does show some signs of being significatnly worse for Texas than the other years.  \n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1493561310661.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2458729201",
    "id" : "D79EB0BB",
    "lastKnownWriteTime" : 1494173239,
    "last_content_update" : 1494173239638,
    "path" : "C:/SMU Data science/Statistical Sampling/homework/Unit 1/Homework_1_code.Rmd",
    "project_path" : "Homework_1_code.Rmd",
    "properties" : {
        "last_setup_crc32" : "C7E16C12bb338d19",
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}